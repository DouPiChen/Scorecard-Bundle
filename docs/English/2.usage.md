---
sort: 2
---

# Quick Start

- Like Scikit-Learn, Scorecard-Bundle basiclly have two types of obejects, transforms and predictors. They comply with the fit-transform and fit-predict convention;
- A complete example showing how to build a scorecard with high explainability and good predictability with Scorecard-Bundle can be found in [Example Notebooks](http://scorecard-bundle.bubu.blue/Notebooks/);
- See more details in [API Reference](http://scorecard-bundle.bubu.blue/API/);


## Load Scorecard-Bundle

  ```python
  from scorecardbundle.feature_discretization import ChiMerge as cm
  from scorecardbundle.feature_discretization import FeatureIntervalAdjustment as fia
  from scorecardbundle.feature_encoding import WOE as woe
  from scorecardbundle.feature_selection import FeatureSelection as fs
  from scorecardbundle.model_training import LogisticRegressionScoreCard as lrsc
  from scorecardbundle.model_evaluation import ModelEvaluation as me
  ```

## Feature Discretization (ChiMerge)

  Note that the feature intervals here are open to the left and close to the right.

  ```python
  trans_cm = cm.ChiMerge(max_intervals=10, min_intervals=5, output_dataframe=True)
  result_cm = trans_cm.fit_transform(X, y) 
  trans_cm.boundaries_ # see the interval binnign for each feature
  ```

## Feature Encoding (WOE) and Evaluation (IV)

  ```python
  trans_woe = woe.WOE_Encoder(output_dataframe=True)
  result_woe = trans_woe.fit_transform(result_cm, y)
  print(trans_woe.iv_) # information value (iv) for each feature
  print(trans_woe.result_dict_) # woe dictionary and iv value for each feature
  ```

## Feature Engineering

  Check the sample distribution and event rate distribution for each feature. Adjust the feature intervals to get better interpretability or stability.

### Check the sample distribution and event rate distribution 

    ```python
    col = 'housing_median_age'
    fia.plot_event_dist(result_cm[col],y,x_rotation=60)
    ```

<img src="../pics/fe_eg1.PNG">

### Adjust the feature intervals to get better interpretability or stability

    ```python
    new_x = cm.assign_interval_str(X[col].values,[33,45]) # apply new interval boundaries to the feature
    woe.woe_vector(new_x, y.values) # check the information value of the resulted feature that applied the new intervals
    ```

    ```
    ({'-inf~33.0': -0.21406011499136973,
      '33.0~45.0': 0.08363199161936338,
      '45.0~inf': 0.7012457415969229},
     0.09816803871772663)
    ```

### Check the distributions again

    ```python
    fia.plot_event_dist(new_x,y,title=f"Event rate distribution of feature '{col}'", x_label=col,
                        y_label='More valuable than Q90',
                       x_rotation=60,save=True)
    ```

<img src="../pics/fe_eg2.PNG">

### Update the dataset of binned features

    ```python
    result_cm[col] = new_x # great explainability and predictability. Select.
    feature_list.append(col)
    print(feature_list)
    ```

### WOE on the interval-adjusted feature data

After finishing interval adjustments for all features, perform WOE encoding to the adjusted feature data

  ```python
  trans_woe = woe.WOE_Encoder(output_dataframe=True)
  result_woe = trans_woe.fit_transform(result_cm[feature_list], y) 
  result_woe.head()
  ```

  |      | latitude  | median_income | total_rooms | housing_median_age | longitude | population |
  | :--- | :-------- | :------------ | :---------- | :----------------- | :-------- | ---------- |
  | 0    | -0.126659 | -0.295228     | 0.294410    | 0.083632           | -0.37460  | 0.044972   |
  | 1    | 0.300017  | -0.295228     | 0.294410    | -0.214060          | -0.37460  | -0.262287  |
  | 2    | -0.126659 | -1.056799     | -0.236322   | 0.083632           | -0.37460  | -0.262287  |
  | 3    | -0.447097 | -0.295228     | 0.294410    | -0.214060          | 0.68479   | 0.044972   |
  | 4    | 0.300017  | -2.194393     | -0.236322   | 0.083632           | -0.37460  | -0.262287  |

  ```python
  trans_woe.iv_ # the information value (iv) for each feature
  ```

  ```
  {'latitude': 0.09330096146239328,
   'median_income': 2.5275362958451018,
   'total_rooms': 0.12825413939140448,
   'housing_median_age': 0.09816803871772663,
   'longitude': 0.11101533122863683,
   'population': 0.07193530955126093}
  ```

- Feature selection

  remove features with insufficient information（IV less than 0.02）, or the feature which are highly correlated with other features with stronger iv to avoid colinearity problem. (the correlation here is measured by Pearson Correlation Coefficient with threshold 0.6, which can be adjusted by parameter `threshold_corr`)

  ```python
  fs.selection_with_iv_corr(trans_woe, result_woe) # column 'corr_with' lists the other features that are highly correlated with the feature 
  ```

  |      | factor             | IV       | woe_dict                                          | corr_with                           |
  | :--- | :----------------- | :------- | :------------------------------------------------ | ----------------------------------- |
  | 1    | median_income      | 2.527536 | {'-inf~2.875': -2.1943934506450016, '2.875~3.5... | {}                                  |
  | 2    | total_rooms        | 0.128254 | {'-inf~1176.0': -0.7003111163731507, '1176.0~2... | {'population': -0.708264369318934}  |
  | 4    | longitude          | 0.111015 | {'-118.37~inf': -0.3746002086398019, '-122.41~... | {}                                  |
  | 3    | housing_median_age | 0.098168 | {'-inf~33.0': -0.21406011499136973, '33.0~45.0... | {}                                  |
  | 0    | latitude           | 0.093301 | {'-inf~34.0': -0.12665884912551398, '34.0~37.6... | {}                                  |
  | 5    | population         | 0.071935 | {'-inf~873.0': 0.2876654505272535, '1275.0~152... | {'total_rooms': -0.708264369318934} |

  ```python
  # "total_rooms" and "population" are high correlated. Drop "population" since it has lower IV.
  feature_list.remove("population")
  feature_list
  ```

  ```python
  # Perform WOE encoding again
  trans_woe = woe.WOE_Encoder(output_dataframe=True)
  result_woe = trans_woe.fit_transform(result_cm[feature_list], y) # WOE is fast. This only takes less then 1 seconds
  result_woe.head()
  ```

  ```python
  # Check correlation heat map
  corr_matrix = result_woe.corr()
  plt.figure(figsize=(3,3))
  sns.heatmap(corr_matrix, cmap = 'bwr', center=0)
  plt.xticks(fontsize=12)
  plt.yticks(fontsize=12)
  plt.show()
  ```

<img src="../pics/heatmap.PNG">

## Model Training: Train the Scorecard model and get the scoring rules. 

  Note that the feature intervals (`value` column in the rules table) are all open to the left and close to the right (e.g. 34.0~37.6 means (34.0, 37.6] ). 

  ```python
  model = lrsc.LogisticRegressionScoreCard(trans_woe, PDO=-20, basePoints=100, verbose=True)
  model.fit(result_woe, y)
  model.woe_df_ # the scorecard rules
  '''
  	feature: feature name;
  	value: feature intervals that are open to the left and closed to the right;
  	woe: the woe encoding for each feature interval;
  	beta: the regression coefficients for each feature in the Logistic regression;
  	score: the assigned score for each feature interval;
  '''
  ```

  |      | feature            | value                      | woe       | beta     | score |
  | :--- | :----------------- | :------------------------- | :-------- | :------- | :---- |
  | 0    | latitude           | -inf~34.0                  | -0.126659 | 1.443634 | 15.0  |
  | 1    | latitude           | 34.0~37.6                  | 0.300017  | 1.443634 | 32.0  |
  | 2    | latitude           | 37.6~inf                   | -0.447097 | 1.443634 | 1.0   |
  | 3    | median_income      | -inf~2.875                 | -2.194393 | 1.078135 | -48.0 |
  | 4    | median_income      | 2.875~3.5625               | -1.056799 | 1.078135 | -13.0 |
  | 5    | median_income      | 3.5625~3.9625              | -0.681424 | 1.078135 | -1.0  |
  | 6    | median_income      | 3.9625~5.102880000000001   | -0.295228 | 1.078135 | 11.0  |
  | 7    | median_income      | 5.102880000000001~5.765463 | 0.388220  | 1.078135 | 32.0  |
  | 8    | median_income      | 5.765463~6.340365          | 0.909407  | 1.078135 | 48.0  |
  | 9    | median_income      | 6.340365~6.953202          | 1.355401  | 1.078135 | 62.0  |
  | 10   | median_income      | 6.953202~7.737496000000001 | 1.895717  | 1.078135 | 79.0  |
  | 11   | median_income      | 7.737496000000001~8.925106 | 3.130872  | 1.078135 | 117.0 |
  | 12   | median_income      | 8.925106~inf               | 4.940572  | 1.078135 | 173.0 |
  | 13   | total_rooms        | -inf~1176.0                | -0.700311 | 0.792079 | 4.0   |
  | 14   | total_rooms        | 1176.0~2012.0              | -0.236322 | 0.792079 | 14.0  |
  | 15   | total_rooms        | 2012.0~2499.0              | -0.022575 | 0.792079 | 19.0  |
  | 16   | total_rooms        | 2499.0~4178.0              | 0.294410  | 0.792079 | 27.0  |
  | 17   | total_rooms        | 4178.0~inf                 | 0.430445  | 0.792079 | 30.0  |
  | 18   | housing_median_age | -inf~33.0                  | -0.214060 | 2.105821 | 7.0   |
  | 19   | housing_median_age | 33.0~45.0                  | 0.083632  | 2.105821 | 25.0  |
  | 20   | housing_median_age | 45.0~inf                   | 0.701246  | 2.105821 | 62.0  |
  | 21   | longitude          | -118.37~inf                | -0.374600 | 1.451295 | 4.0   |
  | 22   | longitude          | -122.41~-118.37            | 0.152499  | 1.451295 | 26.0  |
  | 23   | longitude          | -inf~-122.41               | 0.684790  | 1.451295 | 48.0  |

## Scorecard Adjustment: Users can manually modify the scorecard rules (with codes as shown bellow, or save the rules table locally, finish modification and load the excel file to kernal），and then use the `load_scorecard` parameter of `predict()` function to pass the scorecard rules to model.See details in the API doc of `load_scorecard` parameter.

  ```python
  sc_table = model.woe_df_.copy()
  sc_table['score'][(sc_table.feature=='housing_median_age') & (sc_table.value=='45.0~inf')] = 61
  sc_table
  ```

- Apply the Scorecard

  Scorecard should be applied on the original feature values, namely the features before discretization and WOE encoding.

  ```python
  result = model.predict(X[feature_list], load_scorecard=sc_table) # Scorecard should be applied on the original feature values
  result_val = model.predict(X_val[feature_list], load_scorecard=sc_table) # Scorecard should be applied on the original feature values
  result.head() # if model object's verbose parameter is set to False, predict will only return Total scores
  ```

  |      | latitude | median_income | total_rooms | housing_median_age | longitude | TotalScore |
  | :--- | :------- | :------------ | :---------- | :----------------- | :-------- | ---------- |
  | 0    | 15.0     | 11.0          | 27.0        | 25.0               | 4.0       | 82.0       |
  | 1    | 32.0     | 11.0          | 27.0        | 7.0                | 4.0       | 81.0       |
  | 2    | 15.0     | -13.0         | 14.0        | 25.0               | 4.0       | 45.0       |
  | 3    | 1.0      | 11.0          | 27.0        | 7.0                | 48.0      | 94.0       |
  | 4    | 32.0     | -48.0         | 14.0        | 25.0               | 4.0       | 27.0       |

  Loading a scorecard rules file from local position to a new kernal:

  ```python
  # OR if we load rules from local position.
  sc_table = pd.read_excel('rules')
  
  model = lrsc.LogisticRegressionScoreCard(woe_transformer=None, verbose=True) # Pass None to woe_transformer because the predict function does not need it
  result = model.predict(X[feature_list], load_scorecard=sc_table) # Scorecard should be applied on the original feature values
  result_val = model.predict(X_val[feature_list], load_scorecard=sc_table) # Scorecard should be applied on the original feature values
  result.head() # if model object's verbose parameter is set to False, predict will only return Total scores
  ```

## Model Evaluation

  suitable for any binary classification task.

  ```python
  # Train
  evaluation = me.BinaryTargets(y, result['TotalScore'])
  evaluation.plot_all()
  
  # Validation
  evaluation = me.BinaryTargets(y_val, result_val['TotalScore'])
  evaluation.plot_all()
  ```

<img src="../pics/eval.PNG">

